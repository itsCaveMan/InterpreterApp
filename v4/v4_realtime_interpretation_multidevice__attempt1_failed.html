<!--

    DOCUMENTATION:
    - The purpose of this page is to interpret the users speech from one language to another language.
    - This page makes use of the device's microphone and speaker.
    - The page creates a session with OpenAI Realtime to handle the interpretation. 
    
    - OPENAI REALTIME DOCUMENTATION:
        - The Realtime turn detection is manual, and handled by this page.
        - This page controls who's turn it is to speak (between the user input or Realtime response).
        - This page captures all the micrphone input, and keeps it in a buffer.
        - The page creates a que of blocks(sentences/phrases) from the user's speech to be sent to OpenAI Realtime.
        - When the response from 1 block is finished, the page sends the next block to OpenAI Realtime.

    THE BARGIN PROBLEM: 
    - the Realtime service will cut off if the user starts talking. This is called bargin. it is a result of auto matic turn detection. 
    - An example is: A user speaks a sentence and it's sent to Realtime. Realtime starts streaming the response, however as the user starts speaking agian, the Realtime stream is cut. The user "barged in" 
    - this results in a very unsmooth interpretation experience. 
    - The goal is that the user can continuously speak comfortably, and this page will dynamically handle the audio chunks + Realtime so that all the audio is A) sent to Realtime, and B) the response is played out completely. 


-->


<!-- Working realtime. however bargin is not solved here -->
<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Real-time Translation Service</title>
    <script src="https://cdn.tailwindcss.com"></script>

    <script>
        window.translatorComponent = function () {
            return {
                session: null,
                localStream: null,
                isConnected: false,
                apiKey: '',
                prompt: 'Translate this English to Korean',
                statusMessage: 'Ready to start',
                isBusy: false,

                // Manual turn + mic buffering state
                audioContext: null,
                micSource: null,
                processorNode: null,
                currentSamples: [], // Float32 samples accumulating for current spoken block
                micQueue: [], // Array of { base64, createdAtMs }
                isVoiceActive: false,
                lastVoiceTimeMs: 0,
                lastSilenceStartMs: 0,
                isModelStreaming: false,
                schedulerTimer: null,
                audioSampleRate: 0,
                lastRms: 0,

                // Diagnostics / stats
                recentEvents: [], // push last N event types
                lastEventType: '',
                lastErrorMessage: '',
                stats: {
                    blocksQueuedTotal: 0,
                    blocksSentTotal: 0,
                    blocksDequeuedTotal: 0,
                    responsesCompleted: 0,
                    responsesCanceled: 0,
                    lastDequeueAt: 0,
                    lastSendAt: 0,
                    lastResponseAt: 0,
                },

                // Tunables
                silenceThresholdRms: 0.01, // below this is silence
                minSilenceMsToSplit: 800, // end block after this much silence
                minVoiceMsToStart: 120, // require sustained voice
                maxBlockMs: 15000, // safeguard to cut overly long blocks
                dropBlockAfterMs: 5000, // optional: drop old queued blocks

                init() {
                    const savedApiKey = localStorage.getItem('openai-api-key');
                    const savedPrompt = localStorage.getItem('translation-prompt');
                    if (savedApiKey) this.apiKey = savedApiKey;
                    if (savedPrompt) this.prompt = savedPrompt;
                    this.$watch('apiKey', (v) => localStorage.setItem('openai-api-key', v));
                    this.$watch('prompt', (v) => localStorage.setItem('translation-prompt', v));
                },

                updateStatus(message) {
                    this.statusMessage = message;
                },

                async toggleSession() {
                    if (!this.isConnected) {
                        await this.startSession();
                    } else {
                        this.stopSession();
                    }
                },

                async startSession() {
                    const apiKey = (this.apiKey || '').trim();
                    const prompt = (this.prompt || '').trim();
                    if (!apiKey) {
                        this.updateStatus('Please enter your OpenAI API key');
                        return;
                    }
                    if (!prompt) {
                        this.updateStatus('Please enter a translation prompt');
                        return;
                    }

                    try {
                        this.isBusy = true;
                        this.updateStatus('Requesting microphone...');

                        this.localStream = await navigator.mediaDevices.getUserMedia({
                            audio: {
                                echoCancellation: true,
                                noiseSuppression: true,
                                autoGainControl: true
                            }
                        });

                        // Start local mic analysis and buffering
                        await this.startMicProcessing();

                        // Create a recv-only session for output audio and data channel for control
                        await this.createSession(apiKey, prompt);

                        // Start scheduler to feed queue
                        this.schedulerTimer = setInterval(() => this.tickScheduler(), 100);

                        this.isBusy = false;
                    } catch (error) {
                        console.error('Error starting session:', error);
                        this.updateStatus('Error: ' + (error && error.message ? error.message : String(error)));
                        this.isBusy = false;
                        this.stopSession();
                    }
                },

                async startMicProcessing() {
                    if (!this.localStream) return;
                    this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    this.micSource = this.audioContext.createMediaStreamSource(this.localStream);
                    // Use ScriptProcessorNode for simplicity
                    const bufferSize = 4096;
                    this.processorNode = this.audioContext.createScriptProcessor(bufferSize, 1, 1);
                    const sampleRate = this.audioContext.sampleRate; // often 48000
                    this.audioSampleRate = sampleRate;
                    let voiceStartedAtMs = 0;
                    let lastBlockStartMs = performance.now();
                    this.processorNode.onaudioprocess = (event) => {
                        const input = event.inputBuffer.getChannelData(0);
                        // Compute RMS to detect voice
                        let sumSquares = 0;
                        for (let i = 0; i < input.length; i++) {
                            const v = input[i];
                            sumSquares += v * v;
                        }
                        const rms = Math.sqrt(sumSquares / input.length);
                        this.lastRms = rms;
                        const nowMs = performance.now();

                        // Accumulate samples regardless; we will segment on silence
                        // Copy samples to avoid holding the underlying buffer
                        const copy = new Float32Array(input.length);
                        copy.set(input);
                        this.currentSamples.push(copy);

                        if (rms >= this.silenceThresholdRms) {
                            if (!this.isVoiceActive) {
                                // voice potentially starting
                                if (voiceStartedAtMs === 0) voiceStartedAtMs = nowMs;
                                if ((nowMs - voiceStartedAtMs) >= this.minVoiceMsToStart) {
                                    this.isVoiceActive = true;
                                    this.updateStatus('Listening...');
                                }
                            }
                            this.lastVoiceTimeMs = nowMs;
                        } else {
                            // silence
                            if (this.isVoiceActive) {
                                if (this.lastSilenceStartMs === 0) this.lastSilenceStartMs = nowMs;
                                const silenceDur = nowMs - this.lastSilenceStartMs;
                                if (silenceDur >= this.minSilenceMsToSplit) {
                                    // finalize block
                                    const block = this.finalizeCurrentBlock(sampleRate);
                                    if (block) {
                                        this.micQueue.push(block);
                                        this.stats.blocksQueuedTotal++;
                                        this.trimOldQueuedBlocks();
                                        this.updateStatus(`Queued ${this.micQueue.length} block(s)`);
                                    }
                                    this.isVoiceActive = false;
                                    voiceStartedAtMs = 0;
                                    this.lastSilenceStartMs = 0;
                                    lastBlockStartMs = nowMs;
                                }
                            } else {
                                // idle silence; reset transient voice
                                voiceStartedAtMs = 0;
                            }
                        }

                        // Safety cut for very long block
                        if (this.isVoiceActive && (nowMs - lastBlockStartMs) > this.maxBlockMs) {
                            const block = this.finalizeCurrentBlock(sampleRate);
                            if (block) {
                                this.micQueue.push(block);
                                this.stats.blocksQueuedTotal++;
                                this.trimOldQueuedBlocks();
                                this.updateStatus(`Queued ${this.micQueue.length} block(s)`);
                            }
                            this.isVoiceActive = false;
                            voiceStartedAtMs = 0;
                            this.lastSilenceStartMs = 0;
                            lastBlockStartMs = nowMs;
                        }
                    };
                    this.micSource.connect(this.processorNode);
                    this.processorNode.connect(this.audioContext.destination);
                },

                stopMicProcessing() {
                    try {
                        if (this.processorNode) this.processorNode.disconnect();
                    } catch (_) {}
                    try {
                        if (this.micSource) this.micSource.disconnect();
                    } catch (_) {}
                    try {
                        if (this.audioContext) this.audioContext.close();
                    } catch (_) {}
                    this.processorNode = null;
                    this.micSource = null;
                    this.audioContext = null;
                    this.currentSamples = [];
                },

                finalizeCurrentBlock(sampleRate) {
                    if (!this.currentSamples.length) return null;
                    // Flatten Float32 chunks
                    let totalLen = 0;
                    for (const chunk of this.currentSamples) totalLen += chunk.length;
                    const merged = new Float32Array(totalLen);
                    let offset = 0;
                    for (const chunk of this.currentSamples) {
                        merged.set(chunk, offset);
                        offset += chunk.length;
                    }
                    this.currentSamples = [];
                    // Downsample to 24000 Hz (simple decimation for speed)
                    const targetRate = 24000;
                    const ratio = sampleRate / targetRate;
                    const outLen = Math.floor(merged.length / ratio);
                    const down = new Float32Array(outLen);
                    let j = 0;
                    for (let i = 0; i < outLen; i++) {
                        down[i] = merged[Math.floor(j)];
                        j += ratio;
                    }
                    // Convert to 16-bit PCM
                    const pcm16 = new Int16Array(down.length);
                    for (let i = 0; i < down.length; i++) {
                        let s = Math.max(-1, Math.min(1, down[i]));
                        pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                    }
                    // Base64 encode raw PCM16
                    const base64 = this.int16ToBase64(pcm16);
                    return { base64, createdAtMs: Date.now() };
                },

                int16ToBase64(int16) {
                    // Convert Int16Array to base64 string
                    let binary = '';
                    const bytes = new Uint8Array(int16.buffer);
                    const len = bytes.byteLength;
                    for (let i = 0; i < len; i++) {
                        binary += String.fromCharCode(bytes[i]);
                    }
                    return btoa(binary);
                },

                trimOldQueuedBlocks() {
                    const now = Date.now();
                    this.micQueue = this.micQueue.filter(b => (now - b.createdAtMs) <= this.dropBlockAfterMs);
                },

                tickScheduler() {
                    if (!this.session || !this.session.dc || this.session.dc.readyState !== 'open') return;
                    if (this.isModelStreaming) return;
                    this.trimOldQueuedBlocks();
                    if (this.micQueue.length === 0) return;
                    // Dequeue and send next block
                    const next = this.micQueue.shift();
                    this.stats.blocksDequeuedTotal++;
                    this.stats.lastDequeueAt = Date.now();
                    this.sendBlockToRealtime(next);
                },

                sendBlockToRealtime(block) {
                    if (!block || !block.base64) return;
                    const dc = this.session.dc;
                    try {
                        // Clear any previous residual input
                        dc.send(JSON.stringify({ type: 'input_audio_buffer.clear' }));
                        // Append in safe chunks to avoid DC size limits
                        const CHUNK_SIZE = 15000; // chars
                        for (let i = 0; i < block.base64.length; i += CHUNK_SIZE) {
                            const chunk = block.base64.slice(i, i + CHUNK_SIZE);
                            dc.send(JSON.stringify({ type: 'input_audio_buffer.append', audio: chunk }));
                        }
                        // Commit and request a response
                        dc.send(JSON.stringify({ type: 'input_audio_buffer.commit' }));
                        dc.send(JSON.stringify({
                            type: 'response.create',
                            response: {
                                instructions: (this.prompt || '').trim(),
                                modalities: ['audio'],
                                audio: { voice: 'alloy' }
                            }
                        }));
                        this.isModelStreaming = true;
                        this.stats.blocksSentTotal++;
                        this.stats.lastSendAt = Date.now();
                        this.updateStatus('Translating...');
                    } catch (err) {
                        console.error('Failed to send block:', err);
                        this.lastErrorMessage = String(err && err.message ? err.message : err);
                    }
                },

                async getClientToken(keyOrToken) {
                    const looksLikeRealKey = keyOrToken.startsWith('sk-');
                    if (!looksLikeRealKey) return keyOrToken;
                    this.updateStatus('Creating ephemeral token...');
                    const response = await fetch('https://api.openai.com/v1/realtime/sessions', {
                        method: 'POST',
                        headers: {
                            'Authorization': `Bearer ${keyOrToken}`,
                            'Content-Type': 'application/json',
                        },
                        body: JSON.stringify({
                            model: 'gpt-4o-realtime-preview',
                            voice: 'alloy'
                        }),
                    });
                    if (!response.ok) {
                        throw new Error(`Failed to create session (${response.status})`);
                    }
                    const data = await response.json();
                    const token = data && data.client_secret && data.client_secret.value;
                    if (!token) {
                        throw new Error('No client_secret.value in session response');
                    }
                    return token;
                },

                async waitForIceGathering(conn) {
                    if (conn.iceGatheringState === 'complete') return;
                    await new Promise((resolve) => {
                        const check = () => {
                            if (conn.iceGatheringState === 'complete') {
                                conn.removeEventListener('icegatheringstatechange', check);
                                resolve();
                            }
                        };
                        conn.addEventListener('icegatheringstatechange', check);
                    });
                },

                async createSession(keyOrToken, prompt) {
                    try {
                        const clientToken = await this.getClientToken(keyOrToken);
                        const pc = new RTCPeerConnection({
                            iceServers: [{ urls: 'stun:stun.l.google.com:19302' }]
                        });

                        // Receive-only for model audio
                        if (pc.addTransceiver) {
                            pc.addTransceiver('audio', { direction: 'recvonly' });
                        }

                        const inboundStream = new MediaStream();
                        pc.ontrack = (event) => {
                            inboundStream.addTrack(event.track);
                            const audioEl = this.$refs.audio;
                            audioEl.srcObject = inboundStream;
                            audioEl.muted = false;
                            audioEl.play();
                        };

                        pc.onconnectionstatechange = () => {
                            console.log(`Session state:`, pc.connectionState);
                            if (['failed', 'disconnected', 'closed'].includes(pc.connectionState)) {
                                this.stopSession();
                            }
                        };

                        const dc = pc.createDataChannel(`oai-events`);
                        dc.onopen = () => {
                            // Use server VAD to satisfy API, but do NOT auto-create responses
                            dc.send(JSON.stringify({
                                type: 'session.update',
                                session: {
                                    instructions: prompt,
                                    turn_detection: { type: 'server_vad', create_response: false },
                                    input_audio_format: { type: 'pcm16', sample_rate: 24000 },
                                    voice: 'alloy',
                                    modalities: ['audio', 'text']
                                }
                            }));
                            this.isConnected = true;
                            this.updateStatus('Connected - Listening');
                        };
                        dc.onerror = (e) => {
                            const message = (e && e.message) ? e.message : 'DataChannel error';
                            console.error('DC error:', e);
                            this.lastErrorMessage = message;
                            this.lastEventType = 'error';
                        };

                        dc.onmessage = (event) => {
                            try {
                                const msg = JSON.parse(event.data);
                                const type = msg && msg.type;
                                this.lastEventType = type || '';
                                this.recentEvents.push(this.lastEventType);
                                if (this.recentEvents.length > 12) this.recentEvents.shift();
                                if (type === 'error') {
                                    const err = msg && (msg.error || msg);
                                    const errMsg = err && (err.message || err.error || JSON.stringify(err));
                                    this.lastErrorMessage = errMsg || 'Unknown server error';
                                    this.updateStatus('Error: ' + this.lastErrorMessage);
                                }
                                if (type === 'response.completed' || type === 'response.canceled') {
                                    this.isModelStreaming = false;
                                    if (type === 'response.completed') this.stats.responsesCompleted++;
                                    if (type === 'response.canceled') this.stats.responsesCanceled++;
                                    this.stats.lastResponseAt = Date.now();
                                    // If queue has items, scheduler will send next on next tick
                                    this.updateStatus('Ready');
                                }
                                // Optional: if we see any response audio starting, mark streaming
                                if (
                                    type === 'response.output_audio.delta' ||
                                    type === 'response.output_audio.start' ||
                                    type === 'response.created' ||
                                    type === 'response.delta'
                                ) {
                                    this.isModelStreaming = true;
                                }
                                if (type === 'response.output_audio.done') {
                                    this.isModelStreaming = false;
                                    this.stats.responsesCompleted++;
                                    this.stats.lastResponseAt = Date.now();
                                }
                            } catch (_) {
                                // ignore non-JSON messages
                            }
                        };

                        const offer = await pc.createOffer({ offerToReceiveAudio: true });
                        await pc.setLocalDescription(offer);
                        await this.waitForIceGathering(pc);

                        const sdpResponse = await fetch('https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview', {
                            method: 'POST',
                            headers: {
                                'Authorization': `Bearer ${clientToken}`,
                                'Content-Type': 'application/sdp',
                            },
                            body: pc.localDescription.sdp,
                        });
                        if (!sdpResponse.ok) {
                            throw new Error(`SDP exchange failed: ${sdpResponse.status} ${sdpResponse.statusText}`);
                        }
                        const answerSdp = await sdpResponse.text();
                        await pc.setRemoteDescription({ type: 'answer', sdp: answerSdp });

                        const audioEl = this.$refs.audio;
                        this.session = { pc, dc, inboundStream, audioElement: audioEl };
                    } catch (error) {
                        console.error('Session connection error:', error);
                        this.updateStatus('Error: ' + (error && error.message ? error.message : String(error)));
                        throw error;
                    }
                },

                stopSession() {
                    this.updateStatus('Disconnecting...');
                    const s = this.session;
                    if (s) {
                        try {
                            if (s.dc && s.dc.readyState === 'open') s.dc.close();
                        } catch (_) {}
                        try {
                            if (s.pc) s.pc.close();
                        } catch (_) {}
                    }
                    this.session = null;
                    if (this.localStream) {
                        this.localStream.getTracks().forEach((track) => track.stop());
                    }
                    this.localStream = null;
                    this.isConnected = false;
                    if (this.schedulerTimer) {
                        clearInterval(this.schedulerTimer);
                        this.schedulerTimer = null;
                    }
                    this.stopMicProcessing();
                    this.resetUI();
                },

                resetUI() {
                    this.isBusy = false;
                    this.isModelStreaming = false;
                    this.micQueue = [];
                    this.currentSamples = [];
                    this.updateStatus('Ready to start');
                },
            };
        };
    </script>

    <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"></script>
</head>
<body class="bg-gray-50 min-h-screen">
    <div class="container mx-auto px-4 py-6 max-w-md" x-data="translatorComponent()">
        <h1 class="text-2xl font-bold text-center mb-6 text-gray-800">
            Real-time Translation
        </h1>
        
        <div class="mb-4">
            <label for="apiKey" class="block text-sm font-medium text-gray-700 mb-2">
                OpenAI API Key
            </label>
            <input 
                type="password" 
                id="apiKey" 
                placeholder="Enter your OpenAI API key"
                class="w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500"
                x-model="apiKey"
            >
        </div>

        <div class="mb-4">
            <label for="prompt" class="block text-sm font-medium text-gray-700 mb-2">
                Translation Prompt
            </label>
            <textarea 
                id="prompt" 
                rows="3"
                placeholder="e.g., Translate this English to Korean"
                class="w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500 resize-none"
                x-model="prompt"
            ></textarea>
        </div>

        <div class="mb-6">
            <button 
                class="w-full bg-blue-600 hover:bg-blue-700 disabled:bg-gray-400 text-white font-medium py-3 px-4 rounded-md transition-colors"
                @click="toggleSession()"
                :disabled="isBusy"
                x-text="isConnected ? 'Stop Translation Session' : 'Start Translation Session'"
            ></button>
        </div>

        <div class="text-center text-sm text-gray-600 mb-4" x-text="statusMessage"></div>

        <audio x-ref="audio" autoplay playsinline class="hidden"></audio>

        <!-- Diagnostics Panel -->
        <div class="mt-6 p-4 border rounded-lg bg-white shadow-sm text-sm space-y-3">
            <div class="font-semibold text-gray-800">Diagnostics</div>
            <div class="grid grid-cols-2 gap-2">
                <div>
                    <div class="text-gray-500">Connection</div>
                    <div class="font-mono">Session: <span x-text="isConnected ? 'connected' : 'disconnected'"></span></div>
                    <div class="font-mono">DC: <span x-text="session && session.dc ? session.dc.readyState : 'n/a'"></span></div>
                </div>
                <div>
                    <div class="text-gray-500">Turn</div>
                    <div class="font-mono">Model streaming: <span x-text="isModelStreaming ? 'model' : 'user'"></span></div>
                </div>
                <div>
                    <div class="text-gray-500">Mic</div>
                    <div class="font-mono">RMS: <span x-text="lastRms.toFixed(3)"></span></div>
                    <div class="font-mono">Sample rate: <span x-text="audioSampleRate"></span> Hz</div>
                    <div class="font-mono">Voice active: <span x-text="isVoiceActive"></span></div>
                </div>
                <div>
                    <div class="text-gray-500">Queue</div>
                    <div class="font-mono">Length: <span x-text="micQueue.length"></span></div>
                    <div class="font-mono">Queued total: <span x-text="stats.blocksQueuedTotal"></span></div>
                    <div class="font-mono">Dequeued total: <span x-text="stats.blocksDequeuedTotal"></span></div>
                    <div class="font-mono">Sent total: <span x-text="stats.blocksSentTotal"></span></div>
                </div>
                <div>
                    <div class="text-gray-500">Responses</div>
                    <div class="font-mono">Completed: <span x-text="stats.responsesCompleted"></span></div>
                    <div class="font-mono">Canceled: <span x-text="stats.responsesCanceled"></span></div>
                </div>
                <div>
                    <div class="text-gray-500">Timestamps</div>
                    <div class="font-mono">Last dequeue: <span x-text="stats.lastDequeueAt"></span></div>
                    <div class="font-mono">Last send: <span x-text="stats.lastSendAt"></span></div>
                    <div class="font-mono">Last response: <span x-text="stats.lastResponseAt"></span></div>
                </div>
            </div>
            <div>
                <div class="text-gray-500">Last event</div>
                <div class="font-mono break-all" x-text="lastEventType || '—'"></div>
            </div>
            <div>
                <div class="text-gray-500">Recent events</div>
                <div class="font-mono break-all" x-text="recentEvents.join(', ') || '—'"></div>
            </div>
            <template x-if="lastErrorMessage">
                <div class="text-red-600 font-mono">Error: <span x-text="lastErrorMessage"></span></div>
            </template>
            <div class="pt-2 border-t">
                <button class="px-3 py-1 text-xs bg-gray-100 hover:bg-gray-200 rounded" @click="recentEvents=[]; lastErrorMessage='';">Clear</button>
            </div>
        </div>
    </div>
</body>
</html>

