<!-- RESULT

- Almost worked

I can briefly speak over the last response, until it eventually bargins in
moving onto another method.


-->


<!doctype html>
<!-- 

This version of the real-time translation app prevents the "barge-in" problem with OpenAI Realtime.

The approach implemented:

1. VAD (Voice Activity Detection) is ON - server detects and commits user speech
2. Auto-response is OFF (create_response: false) - no automatic replies are generated  
3. TTS interruption is OFF (interrupt_response: false) - user speech won't stop current playback
4. Manual response triggering using proper audio buffer events

Key settings:
- turn_detection.type = "server_vad"
- turn_detection.create_response = false  
- turn_detection.interrupt_response = false

Flow:
- User speaks â†’ VAD commits the speech as a user item
- If assistant is speaking: speech is queued, no immediate response, TTS continues uninterrupted
- If assistant is not speaking: response is created immediately  
- When assistant finishes (output_audio_buffer.stopped): queued user input triggers next response

This allows users to speak while the assistant is talking without interrupting the current response.

-->
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Real-time Translation Service</title>
    <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-gray-50 min-h-screen">
    <div class="container mx-auto px-4 py-6 max-w-md">
        <h1 class="text-2xl font-bold text-center mb-6 text-gray-800">
            Real-time Translation
        </h1>
        
        <!-- API Key Input -->
        <div class="mb-4">
            <label for="apiKey" class="block text-sm font-medium text-gray-700 mb-2">
                OpenAI API Key
            </label>
            <input 
                type="password" 
                id="apiKey" 
                placeholder="Enter your OpenAI API key"
                class="w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500"
            >
        </div>

        <!-- Prompt Input -->
        <div class="mb-4">
            <label for="prompt" class="block text-sm font-medium text-gray-700 mb-2">
                Translation Prompt
            </label>
            <textarea 
                id="prompt" 
                rows="3"
                placeholder="e.g., Translate this English to Korean"
                class="w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500 resize-none"
            >Translate this English to Korean</textarea>
        </div>

        <!-- Control Button -->
        <div class="mb-6">
            <button 
                id="startButton"
                class="w-full bg-blue-600 hover:bg-blue-700 disabled:bg-gray-400 text-white font-medium py-3 px-4 rounded-md transition-colors"
            >
                Start Translation Session
            </button>
        </div>

        <!-- Status Display -->
        <div id="status" class="text-center text-sm text-gray-600 mb-4">
            Ready to start
        </div>

        <!-- Audio Elements -->
        <audio id="audioPlayback" autoplay playsinline class="hidden"></audio>
    </div>

    <script>
        class RealtimeTranslator {
            constructor() {
                this.pc = null;              // RTCPeerConnection
                this.dc = null;              // DataChannel for events
                this.localStream = null;     // Microphone stream
                this.isConnected = false;
                
                // State tracking for non-interrupting responses
                this.assistantSpeaking = false;
                this.pendingUserInput = false;
                this.currentResponseId = null;
                
                this.apiKeyInput = document.getElementById('apiKey');
                this.promptInput = document.getElementById('prompt');
                this.startButton = document.getElementById('startButton');
                this.statusDiv = document.getElementById('status');
                this.audioPlayback = document.getElementById('audioPlayback');
                
                this.initializeEventListeners();
            }

            initializeEventListeners() {
                this.startButton.addEventListener('click', () => {
                    if (!this.isConnected) {
                        this.startSession();
                    } else {
                        this.stopSession();
                    }
                });

                // Save to localStorage when inputs change
                this.apiKeyInput.addEventListener('input', () => {
                    localStorage.setItem('openai-api-key', this.apiKeyInput.value);
                });

                this.promptInput.addEventListener('input', () => {
                    localStorage.setItem('translation-prompt', this.promptInput.value);
                });

                // Restore values from localStorage on page load
                this.restoreFromLocalStorage();
            }

            restoreFromLocalStorage() {
                const savedApiKey = localStorage.getItem('openai-api-key');
                const savedPrompt = localStorage.getItem('translation-prompt');

                if (savedApiKey) {
                    this.apiKeyInput.value = savedApiKey;
                }

                if (savedPrompt) {
                    this.promptInput.value = savedPrompt;
                }
            }

            updateStatus(message) {
                this.statusDiv.textContent = message;
            }

            async startSession() {
                const apiKey = this.apiKeyInput.value.trim();
                const prompt = this.promptInput.value.trim();

                if (!apiKey) {
                    this.updateStatus('Please enter your OpenAI API key');
                    return;
                }

                if (!prompt) {
                    this.updateStatus('Please enter a translation prompt');
                    return;
                }

                try {
                    this.startButton.disabled = true;
                    this.updateStatus('Connecting to OpenAI...');

                    await this.connectToOpenAI(apiKey, prompt);

                    this.startButton.textContent = 'Stop Translation Session';
                    this.startButton.disabled = false;

                } catch (error) {
                    console.error('Error starting session:', error);
                    this.updateStatus('Error: ' + error.message);
                    this.startButton.disabled = false;
                }
            }

            // Get ephemeral token if using real API key
            async getClientToken(keyOrToken) {
                const looksLikeRealKey = keyOrToken.startsWith("sk-");
                if (!looksLikeRealKey) return keyOrToken; // already ephemeral

                this.updateStatus('Creating ephemeral token...');
                const response = await fetch("https://api.openai.com/v1/realtime/sessions", {
                    method: "POST",
                    headers: {
                        "Authorization": `Bearer ${keyOrToken}`,
                        "Content-Type": "application/json",
                    },
                    body: JSON.stringify({ 
                        model: "gpt-4o-realtime-preview",
                        voice: "alloy"
                    }),
                });
                
                if (!response.ok) {
                    throw new Error(`Failed to create session (${response.status})`);
                }
                
                const data = await response.json();
                const token = data?.client_secret?.value;
                if (!token) {
                    throw new Error("No client_secret.value in session response");
                }
                return token;
            }

            // Wait for ICE gathering to complete
            async waitForIceGathering(conn) {
                if (conn.iceGatheringState === "complete") return;
                await new Promise((resolve) => {
                    const check = () => {
                        if (conn.iceGatheringState === "complete") {
                            conn.removeEventListener("icegatheringstatechange", check);
                            resolve();
                        }
                    };
                    conn.addEventListener("icegatheringstatechange", check);
                });
            }

            async connectToOpenAI(apiKey, prompt) {
                try {
                    this.updateStatus('Requesting microphone...');
                    
                    // 1) Get microphone access
                    this.localStream = await navigator.mediaDevices.getUserMedia({
                        audio: { 
                            echoCancellation: true, 
                            noiseSuppression: true, 
                            autoGainControl: true 
                        }
                    });

                    // 2) Get ephemeral token if needed
                    const clientToken = await this.getClientToken(apiKey);

                    // 3) Create WebRTC peer connection
                    this.pc = new RTCPeerConnection({ 
                        iceServers: [{ urls: "stun:stun.l.google.com:19302" }] 
                    });

                    // 4) Add microphone track
                    this.localStream.getAudioTracks().forEach(track => {
                        this.pc.addTrack(track, this.localStream);
                    });

                    // 5) Handle incoming audio from OpenAI
                    const inboundStream = new MediaStream();
                    this.pc.ontrack = (event) => {
                        inboundStream.addTrack(event.track);
                        this.audioPlayback.srcObject = inboundStream;
                        this.audioPlayback.play();
                    };

                    // 6) Monitor connection state
                    this.pc.onconnectionstatechange = () => {
                        console.log('Connection state:', this.pc.connectionState);
                        if (["failed", "disconnected", "closed"].includes(this.pc.connectionState)) {
                            this.stopSession();
                        }
                    };

                    // 7) Create data channel for commands
                    this.dc = this.pc.createDataChannel("oai-events");
                    this.dc.onopen = () => {
                        this.isConnected = true;
                        this.updateStatus('Connected - Speak now');
                        
                        // Configure session with custom prompt
                        this.dc.send(JSON.stringify({
                            type: "session.update",
                            session: {
                                instructions: prompt,
                                turn_detection: { 
                                    type: "server_vad", 
                                    create_response: false,      // Disable auto-response to prevent interruption
                                    interrupt_response: false    // Prevent TTS interruption when user speaks
                                },
                                voice: "alloy",
                                modalities: ["audio", "text"]
                            }
                        }));
                    };

                    this.dc.onmessage = (event) => {
                        const message = JSON.parse(event.data);
                        console.log('Received:', message);
                        this.handleRealtimeEvent(message);
                    };

                    // 8) Create SDP offer
                    this.updateStatus('Creating connection...');
                    const offer = await this.pc.createOffer({ offerToReceiveAudio: true });
                    await this.pc.setLocalDescription(offer);
                    await this.waitForIceGathering(this.pc);

                    // 9) Exchange SDP with OpenAI
                    this.updateStatus('Connecting to OpenAI...');
                    const sdpResponse = await fetch(`https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview`, {
                        method: "POST",
                        headers: {
                            "Authorization": `Bearer ${clientToken}`,
                            "Content-Type": "application/sdp",
                        },
                        body: this.pc.localDescription.sdp,
                    });

                    if (!sdpResponse.ok) {
                        throw new Error(`SDP exchange failed: ${sdpResponse.status} ${sdpResponse.statusText}`);
                    }

                    const answerSdp = await sdpResponse.text();
                    await this.pc.setRemoteDescription({ 
                        type: "answer", 
                        sdp: answerSdp 
                    });

                } catch (error) {
                    console.error('Connection error:', error);
                    this.updateStatus('Error: ' + error.message);
                    this.stopSession();
                    throw error;
                }
            }

            stopSession() {
                this.updateStatus('Disconnecting...');
                
                // Close data channel
                if (this.dc && this.dc.readyState === "open") {
                    this.dc.close();
                }
                
                // Close peer connection
                if (this.pc) {
                    this.pc.close();
                }
                
                // Stop microphone stream
                if (this.localStream) {
                    this.localStream.getTracks().forEach(track => track.stop());
                }
                
                // Reset state
                this.pc = null;
                this.dc = null;
                this.localStream = null;
                this.isConnected = false;
                
                // Reset response state
                this.assistantSpeaking = false;
                this.pendingUserInput = false;
                this.currentResponseId = null;
                
                this.resetUI();
            }

            resetUI() {
                this.startButton.textContent = 'Start Translation Session';
                this.startButton.disabled = false;
                this.updateStatus('Ready to start');
            }

            handleRealtimeEvent(message) {
                switch (message.type) {
                    case 'input_audio_buffer.committed':
                        // User speech was detected and committed by VAD
                        console.log('User speech committed');
                        if (this.assistantSpeaking) {
                            // Assistant is still speaking, queue this input
                            this.pendingUserInput = true;
                            this.updateStatus('Listening... (response queued)');
                        } else {
                            // Assistant not speaking, create response immediately
                            this.createResponse();
                        }
                        break;

                    case 'output_audio_buffer.started':
                        // Assistant audio playback started (more reliable than response.created)
                        console.log('Assistant audio playback started');
                        this.assistantSpeaking = true;
                        this.updateStatus('Assistant responding...');
                        break;

                    case 'output_audio_buffer.stopped':
                        // Assistant audio playback stopped naturally
                        console.log('Assistant audio playback stopped');
                        this.assistantSpeaking = false;
                        
                        // If there's pending user input, create the next response
                        if (this.pendingUserInput) {
                            this.pendingUserInput = false;
                            this.updateStatus('Processing next input...');
                            // Small delay to ensure clean transition
                            setTimeout(() => {
                                this.createResponse();
                            }, 50);
                        } else {
                            this.updateStatus('Listening...');
                        }
                        break;

                    case 'output_audio_buffer.cleared':
                        // Assistant audio was interrupted/cleared by new user speech
                        console.log('Assistant audio playback cleared (interrupted)');
                        this.assistantSpeaking = false;
                        
                        // Don't create response immediately - wait for the current speech to be committed
                        // The pending input flag will be set when speech gets committed
                        this.updateStatus('Listening...');
                        break;

                    case 'response.created':
                        // Response generation started (keep for logging)
                        this.currentResponseId = message.response.id;
                        console.log('Response created:', this.currentResponseId);
                        break;

                    case 'response.done':
                        // Response generation finished (keep for logging)
                        console.log('Response generation completed');
                        this.currentResponseId = null;
                        break;

                    case 'error':
                        console.error('OpenAI error:', message.error);
                        this.updateStatus('Error: ' + message.error.message);
                        break;

                    default:
                        // Log other events for debugging
                        if (message.type.startsWith('response.') || 
                            message.type.startsWith('conversation.') ||
                            message.type.startsWith('input_audio_buffer.') ||
                            message.type.startsWith('output_audio_buffer.')) {
                            console.log(`Event: ${message.type}`);
                        }
                        break;
                }
            }

            createResponse() {
                if (!this.dc || this.dc.readyState !== 'open') {
                    console.error('Data channel not open');
                    return;
                }

                // Send response.create to trigger the assistant response
                this.dc.send(JSON.stringify({
                    type: 'response.create',
                    response: {
                        modalities: ['audio', 'text']
                    }
                }));
            }
        }

        // Initialize the translator when the page loads
        document.addEventListener('DOMContentLoaded', () => {
            new RealtimeTranslator();
        });
    </script>
</body>
</html>
